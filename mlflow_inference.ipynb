{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479b1c09-3dfb-45c6-9feb-7233befecb49",
   "metadata": {},
   "source": [
    "### Launch Spark Session via CML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf9c814-4573-4464-980e-4e6dda49a28a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|     bri_ranger_demo|\n",
      "|            cdc_data|\n",
      "|cde_demo_pauldefusco|\n",
      "|   cde_demo_pdefusco|\n",
      "|cde_demo_pdefusco...|\n",
      "|        cde_workshop|\n",
      "|cde_workshop_paul...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "# Sample in-code customization of spark configurations\n",
    "#from pyspark import SparkContext\n",
    "#SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de9e15a-6276-43df-a880-c559a7fc41d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import LongType, IntegerType, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "import dbldatagen as dg\n",
    "import dbldatagen.distributions as dist\n",
    "from dbldatagen import FakerTextFactory, DataGenerator, fakerText\n",
    "\n",
    "class LabeledTextGen:\n",
    "\n",
    "    '''Class to Generate Text Data'''\n",
    "\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def dataGen(self, shuffle_partitions_requested = 8, partitions_requested = 8, data_rows = 10000):\n",
    "\n",
    "        # setup use of Faker\n",
    "        FakerTextUS = FakerTextFactory(locale=['en_US'])\n",
    "\n",
    "        # partition parameters etc.\n",
    "        self.spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "        fakerDataspec = (DataGenerator(self.spark, rows=data_rows, partitions=partitions_requested)\n",
    "                    .withColumnSpec(\"id\", minValue=1, maxValue=data_rows, step=1)\n",
    "                    .withColumn(\"text\", text=FakerTextUS(\"address\"))\n",
    "                    .withColumn(\"label\", \"string\", values=[\"0\", \"1\"],random=True)\n",
    "                    )\n",
    "        df = fakerDataspec.build()\n",
    "                \n",
    "        df = df.withColumn(\"idStr\", F.col(\"id\").cast(StringType()))\\\n",
    "            .drop(\"id\")\\\n",
    "            .withColumnRenamed(\"idStr\", \"id\")\n",
    "     \n",
    "        df = df.withColumn(\"labelStr\", F.col(\"label\").cast(FloatType()))\\\n",
    "            .drop(\"label\")\\\n",
    "            .withColumnRenamed(\"labelStr\", \"label\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c92162-6e70-48ca-b6bd-2d97432a2c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dg = LabeledTextGen(spark)\n",
    "\n",
    "training_df = dg.dataGen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efb547-0a6d-4192-9288-7e5b4a11c68e",
   "metadata": {},
   "source": [
    "### Create and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c07cd9c-6ee9-4522-adc6-ca0f703022a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "import cml.data_v1 as cmldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3068535-0a24-4893-92d4-add8d2df9c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experimentRun(df):\n",
    "\n",
    "    mlflow.set_experiment(\"inference\")\n",
    "    \n",
    "    ### MLFLOW EXPERIMENT RUN\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        maxIter=8\n",
    "        regParam=0.01\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "        lr = LogisticRegression(maxIter=maxIter, regParam=regParam)\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "        model = pipeline.fit(df)\n",
    "\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"regParam\", regParam)\n",
    "\n",
    "        #prediction = model.transform(test)\n",
    "        mlflow.spark.log_model(model, artifact_path=\"artifacts\")\n",
    "\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    experiment_id = mlflow.get_experiment_by_name(\"inference\").experiment_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, run_view_type=1)\n",
    "    \n",
    "    return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99f28a7-4b30-448a-901f-c38f4d062ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/09 00:16:06 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/12/09 00:16:07 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/12/09 00:16:08 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/12/09 00:16:08 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "2023/12/09 00:16:24 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpgc2m9qxf/model, flavor: spark), fall back to return ['pyspark==3.2.3', 'pandas<2']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>params.regParam</th>\n",
       "      <th>params.maxIter</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.engineID</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jnya-231a-5rj0-pc2p</td>\n",
       "      <td>svxe-63lj-gllb-n2yp</td>\n",
       "      <td>EXPERIMENT_RUN_FAILED</td>\n",
       "      <td>/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/jn...</td>\n",
       "      <td>2023-12-09 00:14:21.919896064+00:00</td>\n",
       "      <td>2023-12-09 00:14:21.924000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>710c7231a095ef42b4102f2e7c7f0b57abdd1fa9</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>j0t6i3wyfqccdmo3</td>\n",
       "      <td>/usr/local/lib/python3.9/site-packages/ipykern...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poqe-9vto-wsxr-u1dn</td>\n",
       "      <td>svxe-63lj-gllb-n2yp</td>\n",
       "      <td>EXPERIMENT_RUN_FAILED</td>\n",
       "      <td>/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/po...</td>\n",
       "      <td>2023-12-09 00:14:53.322520064+00:00</td>\n",
       "      <td>2023-12-09 00:15:53.132000+00:00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>710c7231a095ef42b4102f2e7c7f0b57abdd1fa9</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>j0t6i3wyfqccdmo3</td>\n",
       "      <td>/usr/local/lib/python3.9/site-packages/ipykern...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ovr3-iznv-lgjy-4wi8</td>\n",
       "      <td>svxe-63lj-gllb-n2yp</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ov...</td>\n",
       "      <td>2023-12-09 00:16:02.336599040+00:00</td>\n",
       "      <td>2023-12-09 00:16:24.988999936+00:00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>710c7231a095ef42b4102f2e7c7f0b57abdd1fa9</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>j0t6i3wyfqccdmo3</td>\n",
       "      <td>/usr/local/lib/python3.9/site-packages/ipykern...</td>\n",
       "      <td>[{\"run_id\": \"ovr3-iznv-lgjy-4wi8\", \"artifact_p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_id        experiment_id                   status  \\\n",
       "0  jnya-231a-5rj0-pc2p  svxe-63lj-gllb-n2yp    EXPERIMENT_RUN_FAILED   \n",
       "1  poqe-9vto-wsxr-u1dn  svxe-63lj-gllb-n2yp    EXPERIMENT_RUN_FAILED   \n",
       "2  ovr3-iznv-lgjy-4wi8  svxe-63lj-gllb-n2yp  EXPERIMENT_RUN_FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  /home/cdsw/.experiments/svxe-63lj-gllb-n2yp/jn...   \n",
       "1  /home/cdsw/.experiments/svxe-63lj-gllb-n2yp/po...   \n",
       "2  /home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ov...   \n",
       "\n",
       "                           start_time                            end_time  \\\n",
       "0 2023-12-09 00:14:21.919896064+00:00    2023-12-09 00:14:21.924000+00:00   \n",
       "1 2023-12-09 00:14:53.322520064+00:00    2023-12-09 00:15:53.132000+00:00   \n",
       "2 2023-12-09 00:16:02.336599040+00:00 2023-12-09 00:16:24.988999936+00:00   \n",
       "\n",
       "  params.regParam params.maxIter tags.mlflow.user  \\\n",
       "0            None           None      pauldefusco   \n",
       "1            0.01              8      pauldefusco   \n",
       "2            0.01              8      pauldefusco   \n",
       "\n",
       "              tags.mlflow.source.git.commit tags.mlflow.source.type  \\\n",
       "0  710c7231a095ef42b4102f2e7c7f0b57abdd1fa9                   LOCAL   \n",
       "1  710c7231a095ef42b4102f2e7c7f0b57abdd1fa9                   LOCAL   \n",
       "2  710c7231a095ef42b4102f2e7c7f0b57abdd1fa9                   LOCAL   \n",
       "\n",
       "      tags.engineID                            tags.mlflow.source.name  \\\n",
       "0  j0t6i3wyfqccdmo3  /usr/local/lib/python3.9/site-packages/ipykern...   \n",
       "1  j0t6i3wyfqccdmo3  /usr/local/lib/python3.9/site-packages/ipykern...   \n",
       "2  j0t6i3wyfqccdmo3  /usr/local/lib/python3.9/site-packages/ipykern...   \n",
       "\n",
       "                       tags.mlflow.log-model.history  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  [{\"run_id\": \"ovr3-iznv-lgjy-4wi8\", \"artifact_p...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentRun(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bba4c5-176f-4a2a-9c67-73d75c03531e",
   "metadata": {},
   "source": [
    "### Create Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd8efd8f-b6bf-42d6-b367-caf163c64e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabeledTextInferenceGen:\n",
    "\n",
    "    '''\n",
    "    Class to Generate Text Data\n",
    "    Same data as above but without lable column\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def dataGen(self, shuffle_partitions_requested = 8, partitions_requested = 8, data_rows = 10000):\n",
    "\n",
    "        # setup use of Faker\n",
    "        FakerTextUS = FakerTextFactory(locale=['en_US'])\n",
    "\n",
    "        # partition parameters etc.\n",
    "        self.spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "        fakerDataspec = (DataGenerator(self.spark, rows=data_rows, partitions=partitions_requested)\n",
    "                    .withColumnSpec(\"id\", minValue=1, maxValue=data_rows, step=1)\n",
    "                    .withColumn(\"text\", text=FakerTextUS(\"address\"))\n",
    "                    )\n",
    "        df = fakerDataspec.build()\n",
    "                \n",
    "        df = df.withColumn(\"idStr\", F.col(\"id\").cast(StringType()))\\\n",
    "            .drop(\"id\")\\\n",
    "            .withColumnRenamed(\"idStr\", \"id\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f3807b-71a8-4fc0-82dc-d3fe4e1fe656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dg = LabeledTextInferenceGen(spark)\n",
    "\n",
    "inference_df = dg.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b21318-21ee-43ca-a67d-e9b3ab0813b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                text| id|\n",
      "+--------------------+---+\n",
      "|580 Watson Avenue...|  0|\n",
      "|061 Keith Prairie...|  1|\n",
      "|223 John Camp Sui...|  2|\n",
      "|290 Chad Cape Sui...|  3|\n",
      "|417 Pacheco Sprin...|  4|\n",
      "|2732 Briggs Mount...|  5|\n",
      "|Unit 3618 Box 670...|  6|\n",
      "|4390 Pierce Passa...|  7|\n",
      "|154 Benjamin Squa...|  8|\n",
      "|467 Nicholas Forg...|  9|\n",
      "|249 Davis Loaf Su...| 10|\n",
      "|90120 Parker Key\\...| 11|\n",
      "|18194 Davis Freew...| 12|\n",
      "|00431 Lisa Cliffs...| 13|\n",
      "|152 Alexis Path\\n...| 14|\n",
      "|37300 Christina T...| 15|\n",
      "|88829 Flores Pass...| 16|\n",
      "|934 Gonzalez Mano...| 17|\n",
      "|674 Miller Place ...| 18|\n",
      "|6504 Angela Park\\...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inference_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7074fd-edd4-4be4-b5aa-3b8d384f67f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'id']\n"
     ]
    }
   ],
   "source": [
    "column_names = inference_df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a3dfde1-484a-4b75-a01e-1f017596e11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/09 00:21:24 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.4.1, required: mlflow==2.4)\n",
      " - pandas (current: 2.1.3, required: pandas<2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2023/12/09 00:21:24 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n",
      "2023/12/09 00:21:24 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "23/12/09 00:21:42 WARN TaskSetManager: Lost task 1.0 in stage 48.0 (TID 208) (100.100.37.123 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    loaded_model, _ = SparkModelCache.get_or_load(archive_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/12/09 00:21:58 WARN TaskSetManager: Lost task 7.1 in stage 48.0 (TID 216) (100.100.207.106 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    loaded_model, _ = SparkModelCache.get_or_load(archive_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/12/09 00:22:11 WARN TaskSetManager: Lost task 3.1 in stage 48.0 (TID 222) (100.100.232.110 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    loaded_model, _ = SparkModelCache.get_or_load(archive_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/12/09 00:22:24 WARN TaskSetManager: Lost task 0.2 in stage 48.0 (TID 225) (100.100.117.194 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    loaded_model, _ = SparkModelCache.get_or_load(archive_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/12/09 00:22:29 ERROR TaskSetManager: Task 7 in stage 48.0 failed 4 times; aborting job\n",
      "[Stage 48:>                                                         (0 + 7) / 8]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    loaded_model, _ = SparkModelCache.get_or_load(archive_path)\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 748, in _load_model\n    return PipelineModel.load(model_uri)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n    return cls.read().load(path)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n    metadataStr = sc.textFile(metadataPath, 1).first()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mpyfunc\u001b[38;5;241m.\u001b[39mspark_udf(spark, model_uri\u001b[38;5;241m=\u001b[39mlogged_model)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predict on a Spark DataFrame.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43minference_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    loaded_model, _ = SparkModelCache.get_or_load(archive_path)\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File \"/opt/cmladdons/python/site-packages/mlflow/spark.py\", line 748, in _load_model\n    return PipelineModel.load(model_uri)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n    return cls.read().load(path)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n    metadataStr = sc.textFile(metadataPath, 1).first()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/09 00:22:30 WARN TaskSetManager: Lost task 5.2 in stage 48.0 (TID 229) (100.100.232.107 executor 2): TaskKilled (Stage cancelled)\n",
      "23/12/09 00:22:31 WARN TaskSetManager: Lost task 3.2 in stage 48.0 (TID 230) (100.100.232.110 executor 8): TaskKilled (Stage cancelled)\n",
      "23/12/09 00:22:31 WARN TaskSetManager: Lost task 4.3 in stage 48.0 (TID 236) (100.100.232.109 executor 4): TaskKilled (Stage cancelled)\n",
      "23/12/09 00:22:32 WARN TaskSetManager: Lost task 0.3 in stage 48.0 (TID 233) (100.100.117.194 executor 6): TaskKilled (Stage cancelled)\n",
      "23/12/09 00:22:32 WARN TaskSetManager: Lost task 6.3 in stage 48.0 (TID 234) (100.100.117.206 executor 3): TaskKilled (Stage cancelled)\n",
      "23/12/09 00:22:32 WARN TaskSetManager: Lost task 1.3 in stage 48.0 (TID 232) (100.100.37.123 executor 5): TaskKilled (Stage cancelled)\n",
      "23/12/09 00:22:33 WARN TaskSetManager: Lost task 2.3 in stage 48.0 (TID 235) (100.100.232.105 executor 1): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_model = '/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts'\n",
    "\n",
    "# Load model as a Spark UDF.\n",
    "loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n",
    "\n",
    "# Predict on a Spark DataFrame.\n",
    "inference_df.withColumn('predictions', loaded_model(*column_names)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054489c2-a27a-4cb2-acaa-a0525ba48578",
   "metadata": {},
   "source": [
    "### Using Pandas Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af0f8374-f580-4e56-b05b-75bd913a7c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "iDfPandas = inference_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f93cc86-611a-4e4f-ad3b-c5797a329c40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/09 00:23:27 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.4.1, required: mlflow==2.4)\n",
      " - pandas (current: 2.1.3, required: pandas<2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2023/12/09 00:23:27 INFO mlflow.spark: File '/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m logged_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load model as a PyFuncModel.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogged_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predict on a Pandas DataFrame.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py:597\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings, dst_path)\u001b[0m\n\u001b[1;32m    595\u001b[0m _add_code_from_conf_to_system_path(local_path, conf, code_key\u001b[38;5;241m=\u001b[39mCODE)\n\u001b[1;32m    596\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_path, conf[DATA]) \u001b[38;5;28;01mif\u001b[39;00m (DATA \u001b[38;5;129;01min\u001b[39;00m conf) \u001b[38;5;28;01melse\u001b[39;00m local_path\n\u001b[0;32m--> 597\u001b[0m model_impl \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMAIN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pyfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m predict_fn \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PyFuncModel(model_meta\u001b[38;5;241m=\u001b[39mmodel_meta, model_impl\u001b[38;5;241m=\u001b[39mmodel_impl, predict_fn\u001b[38;5;241m=\u001b[39mpredict_fn)\n",
      "File \u001b[0;32m/opt/cmladdons/python/site-packages/mlflow/spark.py:842\u001b[0m, in \u001b[0;36m_load_pyfunc\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;66;03m# NB: If there is no existing Spark context, create a new local one.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;66;03m# NB: We're disabling caching on the new context since we do not need it and we want to\u001b[39;00m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;66;03m# (e.g. as part of spark_udf).\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     spark \u001b[38;5;241m=\u001b[39m _create_local_spark_session_for_loading_spark_model()\n\u001b[0;32m--> 842\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _PyFuncModelWrapper(spark, \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/cmladdons/python/site-packages/mlflow/spark.py:748\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(model_uri, dfs_tmpdir_base, local_model_path)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_model_databricks(\n\u001b[1;32m    745\u001b[0m         dfs_tmpdir, local_model_path \u001b[38;5;129;01mor\u001b[39;00m _download_artifact_from_uri(model_uri)\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m _HadoopFileSystem\u001b[38;5;241m.\u001b[39mmaybe_copy_from_uri(model_uri, dfs_tmpdir, local_model_path)\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:332\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:256\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 256\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls)\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:525\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:1591\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1591\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_model = '/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict(iDfPandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3cbcb-0144-4d74-b41c-f9970cca95f4",
   "metadata": {},
   "source": [
    "### Loading Directly from /home/cdsw/.experiments with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bc89b64-11df-47a1-a5ea-1de84cd8a9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mPath = '/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26165e42-0124-48dc-91d8-a7f236422a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o802.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:300)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:240)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Input path does not exist: file:/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:274)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read pickled model via pipeline api\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PipelineModel\n\u001b[0;32m----> 3\u001b[0m persistedModel \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictionsDF \u001b[38;5;241m=\u001b[39m persistedModel\u001b[38;5;241m.\u001b[39mtransform(inference_df)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:332\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:256\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 256\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls)\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:525\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:1588\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1590\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:1535\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1535\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1536\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:466\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o802.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:300)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:240)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Input path does not exist: file:/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:274)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "# read pickled model via pipeline api\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "persistedModel = PipelineModel.load(mPath)\n",
    "\n",
    "# predict\n",
    "predictionsDF = persistedModel.transform(inference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c4466-2b46-4dd4-aa3d-df25e7f68477",
   "metadata": {},
   "source": [
    "Trying different folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bbb2da8-6361-4d63-a842-65d7148cf0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/sparkml/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m persistedModel \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictionsDF \u001b[38;5;241m=\u001b[39m persistedModel\u001b[38;5;241m.\u001b[39mtransform(inference_df)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:332\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:256\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 256\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls)\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:525\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:1591\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1591\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "mPath = '/home/cdsw/.experiments/svxe-63lj-gllb-n2yp/ovr3-iznv-lgjy-4wi8/artifacts/artifacts/sparkml/'\n",
    "\n",
    "persistedModel = PipelineModel.load(mPath)\n",
    "\n",
    "# predict\n",
    "predictionsDF = persistedModel.transform(inference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc8b40-c1ea-47e6-b2df-35e22ba62e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
