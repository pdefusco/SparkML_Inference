{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479b1c09-3dfb-45c6-9feb-7233befecb49",
   "metadata": {},
   "source": [
    "### Launch Spark Session via CML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf9c814-4573-4464-980e-4e6dda49a28a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         namespace|\n",
      "+------------------+\n",
      "|           default|\n",
      "|information_schema|\n",
      "|               sys|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "# Sample in-code customization of spark configurations\n",
    "#from pyspark import SparkContext\n",
    "#SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "CONNECTION_NAME = \"se-aw-mdl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5de9e15a-6276-43df-a880-c559a7fc41d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import LongType, IntegerType, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "import dbldatagen as dg\n",
    "import dbldatagen.distributions as dist\n",
    "from dbldatagen import FakerTextFactory, DataGenerator, fakerText\n",
    "\n",
    "class DataGen:\n",
    "\n",
    "    '''Class to Generate Text Data'''\n",
    "\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def dataGen(self, shuffle_partitions_requested = 8, partitions_requested = 8, data_rows = 10000):\n",
    "\n",
    "        # partition parameters etc.\n",
    "        self.spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "        fakerDataspec = (DataGenerator(self.spark, rows=data_rows, partitions=partitions_requested)\n",
    "                    .withColumn(\"col1\", IntegerType(), minValue=1, maxValue=data_rows)\n",
    "                    .withColumn(\"col2\", IntegerType(), minValue=1, maxValue=data_rows)\n",
    "                    .withColumn(\"label\", \"string\", values=[\"0\", \"1\"],random=True)\n",
    "                    )\n",
    "        df = fakerDataspec.build()\n",
    "     \n",
    "        df = df.withColumn(\"labelStr\", F.col(\"label\").cast(FloatType()))\\\n",
    "            .drop(\"label\")\\\n",
    "            .withColumnRenamed(\"labelStr\", \"label\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22c92162-6e70-48ca-b6bd-2d97432a2c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dg = DataGen(spark)\n",
    "\n",
    "training_df = dg.dataGen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efb547-0a6d-4192-9288-7e5b4a11c68e",
   "metadata": {},
   "source": [
    "### Create and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c07cd9c-6ee9-4522-adc6-ca0f703022a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "126ade60-55ab-44f6-b0c1-f79821a9bfaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3068535-0a24-4893-92d4-add8d2df9c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experimentRun(df):\n",
    "\n",
    "    mlflow.set_experiment(\"inference-simple\")\n",
    "    \n",
    "    ### MLFLOW EXPERIMENT RUN\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        maxIter=8\n",
    "        regParam=0.01\n",
    "\n",
    "        assembler = VectorAssembler(inputCols=['col1','col2'], outputCol='features')\n",
    "        lr = LogisticRegression(maxIter=maxIter, regParam=regParam)\n",
    "        \n",
    "        pipeline = Pipeline(stages=[assembler, lr])\n",
    "        model = pipeline.fit(df)\n",
    "\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"regParam\", regParam)\n",
    "\n",
    "        #prediction = model.transform(test)\n",
    "        mlflow.spark.log_model(model, artifact_path=\"artifacts\")\n",
    "\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    experiment_id = mlflow.get_experiment_by_name(\"inference-simple\").experiment_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, run_view_type=1)\n",
    "    \n",
    "    return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f99f28a7-4b30-448a-901f-c38f4d062ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 22:59:02 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/02/26 22:59:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/02/26 22:59:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "2024/02/26 22:59:18 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp5ednkkw9/model, flavor: spark), fall back to return ['pyspark==3.2.3', 'pandas<2']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>params.maxIter</th>\n",
       "      <th>params.regParam</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.engineID</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1be2-gab8-84g9-fe5e</td>\n",
       "      <td>638k-6cau-p4tw-vq6t</td>\n",
       "      <td>EXPERIMENT_RUN_FAILED</td>\n",
       "      <td>/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/1b...</td>\n",
       "      <td>2024-02-26 22:14:54.138494976+00:00</td>\n",
       "      <td>2024-02-26 22:14:54.276999936+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b105e0e52fce135147f398b7f132b9a5abadabb7</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>dpcneccqtnsprhdi</td>\n",
       "      <td>/usr/local/lib/python3.10/site-packages/ipyker...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y8na-mdz6-lvf0-edz0</td>\n",
       "      <td>638k-6cau-p4tw-vq6t</td>\n",
       "      <td>EXPERIMENT_RUN_FAILED</td>\n",
       "      <td>/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/y8...</td>\n",
       "      <td>2024-02-26 22:23:29.904854016+00:00</td>\n",
       "      <td>2024-02-26 22:23:29.908000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b105e0e52fce135147f398b7f132b9a5abadabb7</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>dpcneccqtnsprhdi</td>\n",
       "      <td>/usr/local/lib/python3.10/site-packages/ipyker...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ymb-6808-x13p-7pue</td>\n",
       "      <td>638k-6cau-p4tw-vq6t</td>\n",
       "      <td>EXPERIMENT_RUN_FAILED</td>\n",
       "      <td>/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/4y...</td>\n",
       "      <td>2024-02-26 22:23:35.184467968+00:00</td>\n",
       "      <td>2024-02-26 22:23:35.212000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b105e0e52fce135147f398b7f132b9a5abadabb7</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>dpcneccqtnsprhdi</td>\n",
       "      <td>/usr/local/lib/python3.10/site-packages/ipyker...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2k1-qwzd-szhd-yc5i</td>\n",
       "      <td>638k-6cau-p4tw-vq6t</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/h2...</td>\n",
       "      <td>2024-02-26 22:24:06.357978112+00:00</td>\n",
       "      <td>2024-02-26 22:24:59.124999936+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>b105e0e52fce135147f398b7f132b9a5abadabb7</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>dpcneccqtnsprhdi</td>\n",
       "      <td>/usr/local/lib/python3.10/site-packages/ipyker...</td>\n",
       "      <td>[{\"run_id\": \"h2k1-qwzd-szhd-yc5i\", \"artifact_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lroe-ei98-363k-3761</td>\n",
       "      <td>638k-6cau-p4tw-vq6t</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/lr...</td>\n",
       "      <td>2024-02-26 22:25:53.482353920+00:00</td>\n",
       "      <td>2024-02-26 22:26:55.880999936+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>b105e0e52fce135147f398b7f132b9a5abadabb7</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>dpcneccqtnsprhdi</td>\n",
       "      <td>/usr/local/lib/python3.10/site-packages/ipyker...</td>\n",
       "      <td>[{\"run_id\": \"lroe-ei98-363k-3761\", \"artifact_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kaok-pb97-jmsz-85hr</td>\n",
       "      <td>638k-6cau-p4tw-vq6t</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/ka...</td>\n",
       "      <td>2024-02-26 22:58:36.159972864+00:00</td>\n",
       "      <td>2024-02-26 22:59:19.392999936+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>b105e0e52fce135147f398b7f132b9a5abadabb7</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>dpcneccqtnsprhdi</td>\n",
       "      <td>/usr/local/lib/python3.10/site-packages/ipyker...</td>\n",
       "      <td>[{\"run_id\": \"kaok-pb97-jmsz-85hr\", \"artifact_p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_id        experiment_id                   status  \\\n",
       "0  1be2-gab8-84g9-fe5e  638k-6cau-p4tw-vq6t    EXPERIMENT_RUN_FAILED   \n",
       "1  y8na-mdz6-lvf0-edz0  638k-6cau-p4tw-vq6t    EXPERIMENT_RUN_FAILED   \n",
       "2  4ymb-6808-x13p-7pue  638k-6cau-p4tw-vq6t    EXPERIMENT_RUN_FAILED   \n",
       "3  h2k1-qwzd-szhd-yc5i  638k-6cau-p4tw-vq6t  EXPERIMENT_RUN_FINISHED   \n",
       "4  lroe-ei98-363k-3761  638k-6cau-p4tw-vq6t  EXPERIMENT_RUN_FINISHED   \n",
       "5  kaok-pb97-jmsz-85hr  638k-6cau-p4tw-vq6t  EXPERIMENT_RUN_FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  /home/cdsw/.experiments/638k-6cau-p4tw-vq6t/1b...   \n",
       "1  /home/cdsw/.experiments/638k-6cau-p4tw-vq6t/y8...   \n",
       "2  /home/cdsw/.experiments/638k-6cau-p4tw-vq6t/4y...   \n",
       "3  /home/cdsw/.experiments/638k-6cau-p4tw-vq6t/h2...   \n",
       "4  /home/cdsw/.experiments/638k-6cau-p4tw-vq6t/lr...   \n",
       "5  /home/cdsw/.experiments/638k-6cau-p4tw-vq6t/ka...   \n",
       "\n",
       "                           start_time                            end_time  \\\n",
       "0 2024-02-26 22:14:54.138494976+00:00 2024-02-26 22:14:54.276999936+00:00   \n",
       "1 2024-02-26 22:23:29.904854016+00:00    2024-02-26 22:23:29.908000+00:00   \n",
       "2 2024-02-26 22:23:35.184467968+00:00    2024-02-26 22:23:35.212000+00:00   \n",
       "3 2024-02-26 22:24:06.357978112+00:00 2024-02-26 22:24:59.124999936+00:00   \n",
       "4 2024-02-26 22:25:53.482353920+00:00 2024-02-26 22:26:55.880999936+00:00   \n",
       "5 2024-02-26 22:58:36.159972864+00:00 2024-02-26 22:59:19.392999936+00:00   \n",
       "\n",
       "  params.maxIter params.regParam             tags.mlflow.source.git.commit  \\\n",
       "0           None            None  b105e0e52fce135147f398b7f132b9a5abadabb7   \n",
       "1           None            None  b105e0e52fce135147f398b7f132b9a5abadabb7   \n",
       "2           None            None  b105e0e52fce135147f398b7f132b9a5abadabb7   \n",
       "3              8            0.01  b105e0e52fce135147f398b7f132b9a5abadabb7   \n",
       "4              8            0.01  b105e0e52fce135147f398b7f132b9a5abadabb7   \n",
       "5              8            0.01  b105e0e52fce135147f398b7f132b9a5abadabb7   \n",
       "\n",
       "  tags.mlflow.source.type tags.mlflow.user     tags.engineID  \\\n",
       "0                   LOCAL      pauldefusco  dpcneccqtnsprhdi   \n",
       "1                   LOCAL      pauldefusco  dpcneccqtnsprhdi   \n",
       "2                   LOCAL      pauldefusco  dpcneccqtnsprhdi   \n",
       "3                   LOCAL      pauldefusco  dpcneccqtnsprhdi   \n",
       "4                   LOCAL      pauldefusco  dpcneccqtnsprhdi   \n",
       "5                   LOCAL      pauldefusco  dpcneccqtnsprhdi   \n",
       "\n",
       "                             tags.mlflow.source.name  \\\n",
       "0  /usr/local/lib/python3.10/site-packages/ipyker...   \n",
       "1  /usr/local/lib/python3.10/site-packages/ipyker...   \n",
       "2  /usr/local/lib/python3.10/site-packages/ipyker...   \n",
       "3  /usr/local/lib/python3.10/site-packages/ipyker...   \n",
       "4  /usr/local/lib/python3.10/site-packages/ipyker...   \n",
       "5  /usr/local/lib/python3.10/site-packages/ipyker...   \n",
       "\n",
       "                       tags.mlflow.log-model.history  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3  [{\"run_id\": \"h2k1-qwzd-szhd-yc5i\", \"artifact_p...  \n",
       "4  [{\"run_id\": \"lroe-ei98-363k-3761\", \"artifact_p...  \n",
       "5  [{\"run_id\": \"kaok-pb97-jmsz-85hr\", \"artifact_p...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentRun(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bba4c5-176f-4a2a-9c67-73d75c03531e",
   "metadata": {},
   "source": [
    "### Create Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd8efd8f-b6bf-42d6-b367-caf163c64e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceGen:\n",
    "\n",
    "    '''\n",
    "    Class to Generate Text Data\n",
    "    Same data as above but without lable column\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        \n",
    "    def dataGen(self, shuffle_partitions_requested = 8, partitions_requested = 8, data_rows = 10000):\n",
    "\n",
    "        # partition parameters etc.\n",
    "        self.spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "        fakerDataspec = (DataGenerator(self.spark, rows=data_rows, partitions=partitions_requested)\n",
    "                    .withColumn(\"col1\", IntegerType(), minValue=1, maxValue=data_rows)\n",
    "                    .withColumn(\"col2\", IntegerType(), minValue=1, maxValue=data_rows)\n",
    "                    )\n",
    "        df = fakerDataspec.build()\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6f3807b-71a8-4fc0-82dc-d3fe4e1fe656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dg = InferenceGen(spark)\n",
    "\n",
    "inference_df = dg.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e7074fd-edd4-4be4-b5aa-3b8d384f67f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['col1', 'col2']\n"
     ]
    }
   ],
   "source": [
    "column_names = inference_df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a3dfde1-484a-4b75-a01e-1f017596e11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/26 23:00:08 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n",
      "2024/02/26 23:00:08 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "24/02/26 23:00:29 WARN TaskSetManager: Lost task 1.0 in stage 58.0 (TID 238) (100.100.114.237 executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1275, in udf\n",
      "    loaded_model = mlflow.pyfunc.load_model(local_model_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/02/26 23:00:42 WARN TaskSetManager: Lost task 3.0 in stage 58.0 (TID 240) (100.100.114.237 executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1275, in udf\n",
      "    loaded_model = mlflow.pyfunc.load_model(local_model_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/02/26 23:00:56 WARN TaskSetManager: Lost task 2.1 in stage 58.0 (TID 243) (100.100.114.237 executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1275, in udf\n",
      "    loaded_model = mlflow.pyfunc.load_model(local_model_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/02/26 23:01:09 WARN TaskSetManager: Lost task 2.2 in stage 58.0 (TID 251) (100.100.114.237 executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1275, in udf\n",
      "    loaded_model = mlflow.pyfunc.load_model(local_model_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n",
      "    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n",
      "    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n",
      "    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 748, in _load_model\n",
      "    return PipelineModel.load(model_uri)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n",
      "    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n",
      "    metadataStr = sc.textFile(metadataPath, 1).first()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/02/26 23:01:17 ERROR TaskSetManager: Task 1 in stage 58.0 failed 4 times; aborting job\n",
      "24/02/26 23:01:17 WARN TaskSetManager: Lost task 6.1 in stage 58.0 (TID 258) (100.100.114.248 executor 38): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1275, in udf\n    loaded_model = mlflow.pyfunc.load_model(local_model_path)\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 748, in _load_model\n    return PipelineModel.load(model_uri)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n    return cls.read().load(path)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n    metadataStr = sc.textFile(metadataPath, 1).first()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mpyfunc\u001b[38;5;241m.\u001b[39mspark_udf(spark, model_uri\u001b[38;5;241m=\u001b[39mlogged_model)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predict on a Spark DataFrame.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43minference_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py\", line 1275, in udf\n    loaded_model = mlflow.pyfunc.load_model(local_model_path)\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/spark_model_cache.py\", line 47, in get_or_load\n    SparkModelCache._models[archive_path] = (load_model(local_model_dir), local_model_dir)\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py\", line 597, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 842, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File \"/home/cdsw/.local/lib/python3.10/site-packages/mlflow/spark.py\", line 748, in _load_model\n    return PipelineModel.load(model_uri)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 332, in load\n    return cls.read().load(path)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 256, in load\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py\", line 525, in loadMetadata\n    metadataStr = sc.textFile(metadataPath, 1).first()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1591, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 23:01:18 WARN TaskSetManager: Lost task 7.0 in stage 58.0 (TID 250) (100.100.114.249 executor 39): TaskKilled (Stage cancelled)\n",
      "24/02/26 23:01:19 WARN TaskSetManager: Lost task 4.0 in stage 58.0 (TID 247) (100.100.114.246 executor 36): TaskKilled (Stage cancelled)\n",
      "24/02/26 23:01:19 WARN TaskSetManager: Lost task 5.1 in stage 58.0 (TID 257) (100.100.114.247 executor 37): TaskKilled (Stage cancelled)\n",
      "24/02/26 23:01:19 WARN TaskSetManager: Lost task 2.3 in stage 58.0 (TID 254) (100.100.114.237 executor 27): TaskKilled (Stage cancelled)\n",
      "24/02/26 23:01:19 WARN TaskSetManager: Lost task 3.3 in stage 58.0 (TID 255) (100.100.114.239 executor 29): TaskKilled (Stage cancelled)\n",
      "24/02/26 23:01:20 WARN TaskSetManager: Lost task 0.3 in stage 58.0 (TID 256) (100.100.114.245 executor 35): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_model = '/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/lroe-ei98-363k-3761/artifacts/artifacts'\n",
    "\n",
    "# Load model as a Spark UDF.\n",
    "loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n",
    "\n",
    "# Predict on a Spark DataFrame.\n",
    "inference_df.withColumn('predictions', loaded_model(*column_names)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054489c2-a27a-4cb2-acaa-a0525ba48578",
   "metadata": {},
   "source": [
    "### Using Pandas Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af0f8374-f580-4e56-b05b-75bd913a7c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "iDfPandas = inference_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f93cc86-611a-4e4f-ad3b-c5797a329c40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/26 22:34:50 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.4.1, required: mlflow==2.4)\n",
      " - pandas (current: 2.1.3, required: pandas<2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2024/02/26 22:34:50 INFO mlflow.spark: File '/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/lroe-ei98-363k-3761/artifacts/artifacts/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load model as a PyFuncModel.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogged_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Predict on a Pandas DataFrame.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/opt/cmladdons/python/site-packages/mlflow/pyfunc/__init__.py:597\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings, dst_path)\u001b[0m\n\u001b[1;32m    595\u001b[0m _add_code_from_conf_to_system_path(local_path, conf, code_key\u001b[38;5;241m=\u001b[39mCODE)\n\u001b[1;32m    596\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_path, conf[DATA]) \u001b[38;5;28;01mif\u001b[39;00m (DATA \u001b[38;5;129;01min\u001b[39;00m conf) \u001b[38;5;28;01melse\u001b[39;00m local_path\n\u001b[0;32m--> 597\u001b[0m model_impl \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMAIN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pyfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m predict_fn \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PyFuncModel(model_meta\u001b[38;5;241m=\u001b[39mmodel_meta, model_impl\u001b[38;5;241m=\u001b[39mmodel_impl, predict_fn\u001b[38;5;241m=\u001b[39mpredict_fn)\n",
      "File \u001b[0;32m/opt/cmladdons/python/site-packages/mlflow/spark.py:842\u001b[0m, in \u001b[0;36m_load_pyfunc\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;66;03m# NB: If there is no existing Spark context, create a new local one.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;66;03m# NB: We're disabling caching on the new context since we do not need it and we want to\u001b[39;00m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;66;03m# (e.g. as part of spark_udf).\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     spark \u001b[38;5;241m=\u001b[39m _create_local_spark_session_for_loading_spark_model()\n\u001b[0;32m--> 842\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _PyFuncModelWrapper(spark, \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/cmladdons/python/site-packages/mlflow/spark.py:748\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(model_uri, dfs_tmpdir_base, local_model_path)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_model_databricks(\n\u001b[1;32m    745\u001b[0m         dfs_tmpdir, local_model_path \u001b[38;5;129;01mor\u001b[39;00m _download_artifact_from_uri(model_uri)\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m _HadoopFileSystem\u001b[38;5;241m.\u001b[39mmaybe_copy_from_uri(model_uri, dfs_tmpdir, local_model_path)\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:332\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:256\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 256\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls)\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:525\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:1591\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1591\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict(iDfPandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3cbcb-0144-4d74-b41c-f9970cca95f4",
   "metadata": {},
   "source": [
    "### Loading Directly from /home/cdsw/.experiments with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26165e42-0124-48dc-91d8-a7f236422a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read pickled model via pipeline api\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PipelineModel\n\u001b[0;32m----> 3\u001b[0m persistedModel \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictionsDF \u001b[38;5;241m=\u001b[39m persistedModel\u001b[38;5;241m.\u001b[39mtransform(inference_df)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:332\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:256\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 256\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls)\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:524\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadMetadata\u001b[39m(path, sc, expectedClassName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    Load metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m        If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     metadataPath \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     metadataStr \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(metadataPath, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[1;32m    526\u001b[0m     loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/posixpath.py:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(a, \u001b[38;5;241m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(a)\n\u001b[1;32m     78\u001b[0m     path \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not function"
     ]
    }
   ],
   "source": [
    "# read pickled model via pipeline api\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "persistedModel = PipelineModel.load(loaded_model)\n",
    "\n",
    "# predict\n",
    "predictionsDF = persistedModel.transform(inference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c4466-2b46-4dd4-aa3d-df25e7f68477",
   "metadata": {},
   "source": [
    "Trying different folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bbb2da8-6361-4d63-a842-65d7148cf0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/lroe-ei98-363k-3761/artifacts/artifacts/sparkml/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m persistedModel \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictionsDF \u001b[38;5;241m=\u001b[39m persistedModel\u001b[38;5;241m.\u001b[39mtransform(inference_df)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:332\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:256\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 256\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls)\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:525\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:1591\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1591\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "mPath = '/home/cdsw/.experiments/638k-6cau-p4tw-vq6t/lroe-ei98-363k-3761/artifacts/artifacts/sparkml/'\n",
    "\n",
    "persistedModel = PipelineModel.load(mPath)\n",
    "\n",
    "# predict\n",
    "predictionsDF = persistedModel.transform(inference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc8b40-c1ea-47e6-b2df-35e22ba62e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
